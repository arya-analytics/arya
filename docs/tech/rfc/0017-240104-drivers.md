# 16 - General Purpose Device Drivers

**Feature Name**: General Purpose Device Drivers <br />
**Start Date**: 2024-01-04 <br />
**Authors**: Emiliano Bonilla <br />
**Status**: Draft <br />

# 5 - Detailed Design

## 5.0 - Inbound and Outbound Pipelines

Driver functionality can be separated into two distinct pipelines: outbound and inbound.
Outbound pipelines acquire data from sensors hardware and forward it to Synnax, while
inbound pipelines receive commands from Synnax and execute them on the hardware. This
marks a clear point of design separation, and has a few beneficial properties:

1. Inbound and outbound pipelines don't need to share state (aside from what is
   implemented in hardware specific libraries like DAQmx).
2. The failure of an inbound pipeline should not affect the operation of an outbound
   pipeline, and vice versa.

These two properties reduce the cognitive overhead, as both pipelines can be implemented
independently, only sharing essential primitives. Naturally, each pipeline should be
executed in its own thread.

## 5.1 - Outbound Pipeline Overview

Outbound pipelines operate as a continuous acquisition loop that performs three tasks:

1. Acquire data from hardware.
2. Apply any necessary calibrations, transformations, taring.
3. Write data to Synnax.

This loop should run at a nearly-fixed rate, and only consume a clear amount of CPU,
memory, and DAQ device resources (we'll touch more on timing later). From a high level,
implementing this loop seems quite simple. There are three emergent details that make it
far more difficult:

1. **Error handling** - Happy path execution is simple, but errors can occur in any of
   these steps. Some of these errors are critical and should halt pipeline execution,
   while some are transient and require retries or pipeline restarts. Correctly
   identifying, communicating, and handling these errors requires careful design.

2. **Configuration** - Every pipeline requires detailed configuration information to
   operate: channel names, physical device ports, calibrations, etc. These
   configurations are dynamic, and need to be updated mid-driver operation. Invalid
   configuration parameters are an additional source of errors, and need to be carefully
   communicated to and resolved by the user.

3. **Hardware 'Polymorphism'** - We're aiming to support DAQ hardware from a growing
   number of vendors, and, for the most part, the loop structure remains the same no
   matter the device. Ideally we'd make it possible to keep the majority of the pipeline
   code the same, and develop a standard interface for implementing step #1.

## 5.2 - Inbound Pipeline Overview

## 5.3 - Configuration

There are important properties and patterns to examine.

### 5.3.0 - Properties

#### 5.3.0.0 - Dynamism

Adaptive teams are always making changes to their system configuration: adding and
exchanging sensors, changing calibrations, and swapping DAQ modules. From a user
perspective, these changes are quite frequent. During setup batched configuration
changes can come every few minutes. From a software, perspective, however, these changes
are _very_ infrequent. If a pipeline runs at 200hz and a configuration change comes
every ten minutes, that's 120,000 pipeline iterations between each application. Even
using 100 pipeline iterations (0.5s) to fetch and apply configuration changes would be
almost negligible to operation.

In consequence, it's worthwhile to make the configuration process more expensive to
achieve the following:

1. Improve the configuration process for the user by providing clear, reliable feedback.
2. Improve hot path (i.e. pipeline loop) performance and reduce complexity.

#### 5.3.0.1 - Flexibility

Different organizations configure different hardware in different ways. This makes it
largely impossible to define a fixed schema for configuration parameters; attempting to
do so would not only bloat the codebase, but result in very tight coupling between the
driver, core, and frontend.

#### 5.3.0.2 - Error Variants and Emergence

There are two levels of errors that can occur during configuration: critical and warning.
Critical errors mean the system can't operate, while warnings allow the system to run
but still must be communicated.

The more complex aspect of error handling is that configuration errors can occur at two
points: during pipeline setup and during pipeline execution. For example, the hardware
may validate the acquisition rate, but during runtime an ADC may not be able to keep
pace with the system.

## 5.4 - Key Data Structures

### 5.4.0 - The Rack

A rack represents an independent data acquisition and control process. For example, a
running instance of the driver would represent a single rack in the cluster. Another
example of a rack could be a custom flight computer that communicates directly with
Synnax over the network. Here is the pseudocode for the data structure:

```go
package irrelivant

type Rack struct {
    Key uint32
    Name string
}
```

The `Key` field would represent two composite `uint16s` where the first `uint16` is the
**leaseholder** of the rack i.e. which node the rack writes all of its data to. A rack
can write or read data from channels on any node, but must use this node as its gateway
for all communications. The second uint16 is simply an incrementing counter identifying
a particular rack within the scope of its leaseholder. For example, a rack could be
identified as `Node 5 Rack 2` or `Node 7 Rack 8`. Limiting the number of racks per node
to 65535 is a little bit risky, but it helps make the key much more compact.

### 5.4.1 - The Module

A module is an independent acquisition or control loop within a rack. Each module
communicates with a single device from a single vendor, and operates an inbound or
outbound pipeline. The pipelines of a module have a consistent outbound data acquisition
or outbound acknowledgement rate. For example, a module could represent a continuous
100Hz acquisition process from a National Instruments cDAQ.

By guaranteeing a single sample rate, a single device, and a single vendor, we can
considerably reduce the complexity ceiling of a module. We won't need to write code that
synchronizes timing or configuration between multiple devices from different vendors,
and we can complete all necessary tasks of a module with a single or small number of
threads.

Pseudocode for the data structure is as follows:

```go
package irrelivant

type Module struct {
   Key    uint64
   Name   string
   Type   string
   Config string
}

```

The `Key` field is two composite `uint32s`, where the first `uint32` is the key of a
module's rack, and the second is an incremented counter identifying a rack within
its module. As with a Rack, a module could be identified as `Node 5 Rack 2 Module 5`.

The`Type` field enables different module implementations to operate within the same
rack, and serves as the identifier fora driver side abstract factory to select and
configure the type's module class. The same pattern can be applied for any
module specific interfaces on the frontend.

Config is a JSON encoded string storing the configuration for that type of module. While
it reduces the amount of server side validation that can be completed on a module, it
does enable us to expand the number of DAQ vendors and add custom hardware without
needing to change Synnax Core.

We've debated about whether we should include the `config` field directly on the module
struct or not. Configuration info probably won't be very large (a few hundred kilobytes
at most), and it will be rare to query or list more modules than a rack contains (which
is almost always less than 10). This is a two-way decision whose interface does not
affect the end user, so we can always move the config to its own struct if necessary.

## Working Notes

### How do we communicate module configuration and runtime errors?

1. Rack is in charge of starting, stopping, and
